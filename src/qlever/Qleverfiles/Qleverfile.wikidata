# Qleverfile for Wikidata, use with https://github.com/ad-freiburg/qlever-control
#
# qlever get-data  # downloads two .bz2 files of total size ~100 GB
# qlever index     # takes ~4 hours and ~20 GB RAM (on an AMD Ryzen 9 5900X)
# qlever start     # starts the server (takes a few seconds)

[DEFAULT]
NAME = wikidata

[data]
GET_DATA_URL      = https://dumps.wikimedia.org/wikidatawiki/entities
GET_DATA_CMD      = curl -L -C - --remote-time --remote-name-all ${GET_DATA_URL}/latest-all.ttl.bz2 ${GET_DATA_URL}/latest-lexemes.ttl.bz2 2>&1 && curl -sL ${GET_DATA_URL}/dcatap.rdf | docker run -i --rm -v $$(pwd):/data stain/jena riot --syntax=RDF/XML --output=NT /dev/stdin > dcatap.nt
DATE_WIKIDATA     = $$(date -r latest-all.ttl.bz2 +%d.%m.%Y || echo "NO_DATE")
DATE_WIKIPEDIA    = $$(date -r wikipedia-abstracts.nt +%d.%m.%Y || echo "NO_DATE")
DESCRIPTION       = Full Wikidata dump from ${GET_DATA_URL} (latest-all.ttl.bz2 and latest-lexemes.ttl.bz2, version ${DATE_WIKIDATA}) + English Wikipeda abstracts (version ${DATE_WIKIPEDIA}, available via schema:description)
TEXT_DESCRIPTION  = All English and German literals + all sentences from the English Wikipedia (version ${DATE_WIKIPEDIA}), use with FILTER KEYWORDS(...)

[index]
INPUT_FILES      = latest-all.ttl.bz2 latest-lexemes.ttl.bz2 wikipedia-abstracts.nt dcatap.nt
MULTI_INPUT_JSON = [{ "cmd": "lbzcat -n 4 latest-all.ttl.bz2", "format": "ttl", "parallel": "true" },
                    { "cmd": "lbzcat -n 1 latest-lexemes.ttl.bz2", "format": "ttl", "parallel": "false" },
                    { "cmd": "cat wikipedia-abstracts.nt", "format": "nt", "parallel": "false" },
                    { "cmd": "cat dcatap.nt", "format": "nt", "parallel": "false" }]
SETTINGS_JSON    = { "languages-internal": [], "prefixes-external": [""], "locale": { "language": "en", "country": "US", "ignore-punctuation": true }, "ascii-prefixes-only": true, "num-triples-per-batch": 5000000 }
STXXL_MEMORY     = 10G
TEXT_INDEX       = from_text_records

[server]
PORT                        = 7001
ACCESS_TOKEN                = ${data:NAME}_3fz47hfzrbf64b
MEMORY_FOR_QUERIES          = 40G
CACHE_MAX_SIZE              = 30G
CACHE_MAX_SIZE_SINGLE_ENTRY = 5G
# WARMUP_CMD                  = curl -s http://localhost:${PORT} -H "Accept: application/qlever-results+json" --data-urlencode "query=PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?subject ?label WHERE { ?subject @en@rdfs:label ?label } INTERNAL SORT BY ?subject" --data-urlencode "access-token=${server:ACCESS_TOKEN}" --data-urlencode "pinresult=true" --data-urlencode "send=0" | jq .resultsize | xargs printf "Result size: %'d\n"
TIMEOUT                     = 300s

[runtime]
SYSTEM = native
IMAGE  = adfreiburg/qlever

[ui]
UI_CONFIG = wikidata
